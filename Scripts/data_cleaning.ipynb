{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9478323c-dd2e-4dde-971d-02ead918817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be84561-d288-47e6-b04d-6950972bb847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph\\anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:211: UserWarning: Cell D2858 is marked as a date but the serial value 6684137.0 is outside the limits for dates. The cell will be treated as an error.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../Data/'\n",
    "file_name = 'Max, Samantha, Maria data.xlsx'\n",
    "xls = pd.ExcelFile(data_path + file_name)\n",
    "df_max = pd.read_excel(xls, sheet_name = 'Max', parse_dates = [3])\n",
    "df_mar = pd.read_excel(xls, sheet_name = 'Maria', parse_dates = [3])\n",
    "df_sam = pd.read_excel(xls, sheet_name = 'Samantha', parse_dates = [3])\n",
    "\n",
    "df_all = [df_max, df_mar, df_sam]\n",
    "df_sizes = [df.shape[0] for df in df_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01681203-f577-4055-b2a8-4014338ba893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standize number of columns\n",
    "\n",
    "# Max data set lacks coupon column; dropping\n",
    "df_mar.drop(columns = 'coupon', inplace = True)\n",
    "df_sam.drop(columns = 'Coupon (#)', inplace= True)\n",
    "\n",
    "assert df_max.columns.size == df_mar.columns.size == df_sam.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c363eab-54d4-488f-8ece-d16801dcf21e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standarize column names\n",
    "column_names = ['ID', 'Session', 'ReceiptNum', 'ReceiptDate', 'Item', 'Item2',\n",
    "                 'Uncertain', 'Unknown', 'Quantity', 'Hit', 'Miss', 'Category', 'Comment']\n",
    "for df in df_all:\n",
    "    df.columns = column_names\n",
    "\n",
    "assert df_max.columns.equals(df_mar.columns) and df_mar.columns.equals(df_sam.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6410e51a-bcd5-4a6d-ba3b-1bd4e80f4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values\n",
    "for df in df_all:\n",
    "    # when receipt number is null, assume all items came from a single basket that session\n",
    "    df.loc[:, ['ReceiptNum', 'Quantity']] = df[['ReceiptNum', 'Quantity']].fillna(value = 1)\n",
    "    df.loc[:, ['Item2', 'Comment']] = df[['Item2', 'Comment']].fillna(value = '')\n",
    "    \n",
    "    assert df[['ReceiptNum', 'Quantity', 'Item2', 'Comment']].notna().all(axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb15e33-e3d9-408f-b1e8-955330a706e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign data types \n",
    "\n",
    "# Typos which produced errors during subsequent data type conversion\n",
    "df_sam.loc[df_sam.ReceiptNum == datetime.datetime(1900, 1, 1, 0, 0), 'ReceiptNum'] = 1\n",
    "df_sam.loc[df_sam['Quantity'] == '??', 'Quantity'] = 1\n",
    "\n",
    "string_columns = ['Item', 'Item2', 'Category', 'Comment']\n",
    "for df in df_all:\n",
    "    df.loc[:, ['ID', 'Session', 'ReceiptNum', 'Quantity']] = df[['ID', 'Session', 'ReceiptNum', 'Quantity']].astype(pd.Int16Dtype())\n",
    "    df.loc[:, 'ReceiptDate'] = pd.to_datetime(df['ReceiptDate'], errors = 'coerce')\n",
    "    df.loc[:, string_columns] = df[string_columns].astype(str)\n",
    "    \n",
    "    # clean strings\n",
    "    for col in string_columns:\n",
    "        df.loc[:, col] = df[col].str.lower()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94d566d2-22f4-4dab-9ec2-3d91a322adae",
   "metadata": {
    "tags": []
   },
   "source": [
    "TODO Verify column data types\n",
    "\n",
    "dtypes = df_max.dtypes.to_dict()\n",
    "expected_dtypes = {'ID': pd.Int16Dtype(), 'Session': pd.Int16Dtype(), 'ReceiptNum': pd.Int16Dtype(), 'Quantity': pd.Int16Dtype(),\n",
    "                   'ReceiptDate': np.datetime64(), 'Item': str, 'Item2': str, 'Category': str, 'Comment': str}\n",
    "print(dtypes, expected_dtypes)\n",
    "dtypes == expected_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3d09a1-aa45-475e-b12c-e895cffc7c30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped due to missing essential columns: [0, 0, 0]\n",
      "Number of columns dropped due to item being marked as \"unknown\": [267, 322, 265]\n",
      "Number of duplicate columns dropped: [185, 0, 25]\n",
      "Total number of dropped rows: [452, 322, 290]\n",
      "Drop percentage: [21, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "# Drop unusable rows\n",
    "essential_drop_count, unknown_drop_count, duplicate_drop_count, total_drop_count = [], [], [], []\n",
    "for df in df_all:\n",
    "    # missing essential values\n",
    "    essential_drop_count.append(df[['ID', 'Session', 'ReceiptNum', 'Item']].isna().sum().sum())\n",
    "    df.dropna(subset = ['ID', 'Session', 'ReceiptNum', 'Item'], inplace = True)\n",
    "    \n",
    "    # unreliable\n",
    "    unknown_drop_count.append(df[df['Unknown'] == 'x'].isna().sum().sum())\n",
    "    df.drop(df[df['Unknown'] == 'x'].index, inplace = True)\n",
    "    \n",
    "    # duplicate receipts\n",
    "    duplicate_drop_count.append(df['Comment'].str.contains(r'duplicate|repeat').sum())\n",
    "    df.drop(df[df['Comment'].str.contains(r'duplicate|repeat')].index, inplace = True)\n",
    "    \n",
    "    assert df[['ID', 'Session', 'ReceiptNum', 'Item']].notna().all(axis = None)\n",
    "    assert df_max['Unknown'].isna().all()\n",
    "    assert not df['Comment'].str.contains(r'duplicate|repeat').any()\n",
    "\n",
    "print('Number of columns dropped due to missing essential columns:', essential_drop_count)\n",
    "print('Number of columns dropped due to item being marked as \"unknown\":', unknown_drop_count)\n",
    "print('Number of duplicate columns dropped:', duplicate_drop_count)\n",
    "\n",
    "total_drop_count = [sum(c) for c in zip(essential_drop_count, unknown_drop_count, duplicate_drop_count)]\n",
    "print('Total number of dropped rows:', total_drop_count)\n",
    "\n",
    "drop_percentage = [round(c[0] / c[1] * 100) for c in zip(total_drop_count, df_sizes)]\n",
    "print('Drop percentage:', drop_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ba6587-f049-4835-9972-a496554a0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean data sets\n",
    "df_max.to_csv(data_path + 'clean_max.csv')\n",
    "df_mar.to_csv(data_path + 'clean_mar.csv')\n",
    "df_sam.to_csv(data_path + 'clean_sam.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a81eb391-d162-4056-a955-28a93ca26503",
   "metadata": {
    "tags": []
   },
   "source": [
    "TODO\n",
    "drop after verification?\n",
    "consistency in Category column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
